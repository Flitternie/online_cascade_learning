{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.6365\n",
      "class 0, Total: 495, Accuracy: 33.33%\n",
      "class 1, Total: 521, Accuracy: 88.48%\n",
      "class 2, Total: 486, Accuracy: 73.05%\n",
      "class 3, Total: 498, Accuracy: 58.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lynie/miniconda3/envs/eff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "with open(\"agnews_predictions.txt\") as f:\n",
    "    outputs = [int(line.strip()) for line in f.readlines()]\n",
    "\n",
    "data = datasets.load_from_disk('2000_sampled_agnews')\n",
    "labels = data['label']\n",
    "# calculate overall accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(outputs)):\n",
    "    total += 1\n",
    "    if outputs[i] == labels[i]:\n",
    "        correct += 1\n",
    "print(\"Overall accuracy: {}\".format(correct / total))\n",
    "# calculate accuracy for each of the 4 classes\n",
    "for i in range(4):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j in range(len(outputs)):\n",
    "        if labels[j] == i:\n",
    "            total += 1\n",
    "            if outputs[j] == labels[j]:\n",
    "                correct += 1\n",
    "    print(\"class {}, Total: {}, Accuracy: {:.2f}%\".format(i, total, correct / total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM labels as proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset & library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_697081/221098722.py:5: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-darkgrid')\n"
     ]
    }
   ],
   "source": [
    "# plot the accuracy changes as we adjust the number of samples\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# beautify the plot\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### td-idf as word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "texts = data[\"text\"]  # list of texts to classify\n",
    "labels = data[\"label\"]  # list of texts to classify\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.6514841351074718\n",
      "class 0, Total: 458, Accuracy: 36.03%\n",
      "class 1, Total: 521, Accuracy: 88.48%\n",
      "class 2, Total: 482, Accuracy: 73.65%\n",
      "class 3, Total: 493, Accuracy: 59.23%\n"
     ]
    }
   ],
   "source": [
    "# calculate overall accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(filtered_labels)):\n",
    "    total += 1\n",
    "    if filtered_llama_labels[i] == filtered_labels[i]:\n",
    "        correct += 1\n",
    "print(\"Overall accuracy: {}\".format(correct / total))\n",
    "# calculate accuracy for each of the 4 classes\n",
    "for i in range(4):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j in range(len(filtered_llama_labels)):\n",
    "        if filtered_labels[j] == i:\n",
    "            total += 1\n",
    "            if filtered_llama_labels[j] == filtered_labels[j]:\n",
    "                correct += 1\n",
    "    print(\"class {}, Total: {}, Accuracy: {:.2f}%\".format(i, total, correct / total*100))\n",
    "    \n",
    "baseline_acc = correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5933503836317136\n",
      "class 0, Accuracy: 10.53%\n",
      "class 1, Accuracy: 90.29%\n",
      "class 2, Accuracy: 81.82%\n",
      "class 3, Accuracy: 54.29%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"agnews_predictions.txt\") as f:\n",
    "    llama_labels = [int(line) for line in f.readlines()]\n",
    "\n",
    "filtered_texts, filtered_llama_labels, filtered_labels = [], [], []\n",
    "for t, l, g in zip(texts, llama_labels, labels):\n",
    "    if l < 4:\n",
    "        filtered_texts.append(t)\n",
    "        filtered_llama_labels.append(l)\n",
    "        filtered_labels.append(g)\n",
    "\n",
    "    \n",
    "# Split the data into training and test sets\n",
    "X_train_proxy, X_test_proxy, y_train_proxy, y_test_proxy = train_test_split(filtered_texts, filtered_llama_labels, test_size=0.2, random_state=42)\n",
    "X_train_proxy = vectorizer.fit_transform(X_train_proxy)\n",
    "X_test_proxy = vectorizer.transform(X_test_proxy)\n",
    "\n",
    "_, _, _, y_test_gold = train_test_split(filtered_texts, filtered_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_proxy, y_train_proxy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_proxy = classifier.predict(X_test_proxy)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_gold, y_pred_proxy)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# calculate accuracy for each class\n",
    "for i in range(4):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j in range(len(y_pred_proxy)):\n",
    "        if y_test_gold[j] == i:\n",
    "            total += 1\n",
    "            if y_pred_proxy[j] == y_test_gold[j]:\n",
    "                correct += 1\n",
    "    print(\"class {}, Accuracy: {:.2f}%\".format(i, correct / total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.show_weights(estimator=classifier, \n",
    "                  feature_names= list(vectorizer.get_feature_names_out()),\n",
    "                 top=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentence-bert as word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "texts = data[\"text\"]  # list of texts to classify\n",
    "labels = data[\"label\"]  # list of texts to classify\n",
    "\n",
    "# Encode the training set\n",
    "texts = model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7212276214833759\n",
      "class 0, Accuracy: 41.05%\n",
      "class 1, Accuracy: 96.12%\n",
      "class 2, Accuracy: 82.95%\n",
      "class 3, Accuracy: 67.62%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"agnews_predictions.txt\") as f:\n",
    "    llama_labels = [int(line) for line in f.readlines()]\n",
    "\n",
    "filtered_texts, filtered_llama_labels, filtered_labels = [], [], []\n",
    "for t, l, g in zip(texts, llama_labels, labels):\n",
    "    if l < 4:\n",
    "        filtered_texts.append(t)\n",
    "        filtered_llama_labels.append(l)\n",
    "        filtered_labels.append(g)\n",
    "\n",
    "    \n",
    "# Split the data into training and test sets\n",
    "X_train_proxy, X_test_proxy, y_train_proxy, y_test_proxy = train_test_split(filtered_texts, filtered_llama_labels, test_size=0.2, random_state=42)\n",
    "_, _, _, y_test_gold = train_test_split(filtered_texts, filtered_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_proxy, y_train_proxy)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_proxy = classifier.predict(X_test_proxy)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_gold, y_pred_proxy)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# calculate accuracy for each class\n",
    "for i in range(4):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j in range(len(y_pred_proxy)):\n",
    "        if y_test_gold[j] == i:\n",
    "            total += 1\n",
    "            if y_pred_proxy[j] == y_test_gold[j]:\n",
    "                correct += 1\n",
    "    print(\"class {}, Accuracy: {:.2f}%\".format(i, correct / total*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ - Confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in range(len(y_pred_proxy)):\n",
    "    assert np.argmax(classifier.decision_function(X_test_proxy)[i]) == y_pred_proxy[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a bar chart of the distribution of the confidence scores\n",
    "plt.hist([max(scores) for scores in classifier.decision_function(X_test_proxy)], bins=20)\n",
    "plt.xlabel(\"Confidence score\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = classifier.decision_function(X_test_proxy)\n",
    "acc = []\n",
    "to = []\n",
    "scale = round(max([max(scores) for scores in confidence]))\n",
    "num_steps = 50\n",
    "for k in range(num_steps):\n",
    "    total, count = 0, 0\n",
    "    for i in range(len(y_pred_proxy)):\n",
    "        total += 1\n",
    "        if max(confidence[i]) > k/num_steps*scale:\n",
    "            total += 1\n",
    "            if y_pred_proxy[i] != y_test_gold[i]:\n",
    "                count += 1\n",
    "    to.append(total)\n",
    "    acc.append(1 - count / total)\n",
    "\n",
    "plt.plot([i/num_steps*scale for i in range(num_steps)], acc)\n",
    "plt.xlabel(\"Confidence threshold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Confidence threshold\")\n",
    "plt.gcf().set_size_inches(7, 4)\n",
    "# plt.savefig('confidence.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = classifier.decision_function(X_test_proxy)\n",
    "acc = []\n",
    "to = []\n",
    "scale = round(max([max(scores) for scores in confidence]))\n",
    "for k in range(num_steps):\n",
    "    total, count = 0, 0\n",
    "    for i in range(len(y_pred_proxy)):\n",
    "        total += 1\n",
    "        if max(confidence[i]) > k/num_steps*scale:\n",
    "            label_we_use = y_pred_proxy[i]\n",
    "        else:\n",
    "            label_we_use = y_test_proxy[i]\n",
    "        \n",
    "        if label_we_use == y_test_gold[i]:\n",
    "            count += 1\n",
    "    acc.append(count / total)\n",
    "\n",
    "\n",
    "plt.plot([i/num_steps*scale for i in range(num_steps)], acc)\n",
    "plt.plot([i/num_steps*scale for i in range(num_steps)], [baseline_acc for _ in range(num_steps)], linestyle='--')\n",
    "plt.xlabel(\"Confidence threshold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Overall Accuracy vs. Confidence threshold\")\n",
    "plt.gcf().set_size_inches(7, 4)\n",
    "plt.annotate(f\"Max accuracy: {max(acc)}\", (0.3, max(acc)))\n",
    "\n",
    "# plt.savefig('confidence.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ - Change training data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "X_len = X_train_proxy.shape[0]\n",
    "y_len = len(y_train_proxy)\n",
    "for i in range(1, 80):\n",
    "    # Split the data into training and test sets\n",
    "    X_train_proxy_sampled, y_train_proxy_sampled = X_train_proxy[:min(i*10,X_len)], y_train_proxy[:min(i*10,y_len)]\n",
    "    # Train a logistic regression classifier\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train_proxy_sampled, y_train_proxy_sampled)\n",
    "    \n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_proxy_sampled = classifier.predict(X_test_proxy)\n",
    "\n",
    "    # Evaluate the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test_proxy, y_pred_proxy_sampled)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "plt.plot(np.arange(10, 800, 10), accuracies)\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# set size\n",
    "plt.gcf().set_size_inches(10, 5)\n",
    "plt.savefig('accuracy.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ - Manually adjust label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_idx=8\n",
    "X_train_proxy_sampled, y_train_proxy_sampled = X_train_proxy[:min(num_samples_idx*10,X_len)], y_train_proxy[:min(num_samples_idx*10,y_len)]\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_proxy_sampled, y_train_proxy_sampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_proxy_sampled = classifier.predict(X_test_proxy)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_proxy, y_pred_proxy_sampled)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# calculate accuracy for each class\n",
    "pos_correct = 0\n",
    "pos_total = 0\n",
    "neg_correct = 0\n",
    "neg_total = 0\n",
    "for i in range(len(y_pred_proxy_sampled)):\n",
    "    if y_test_proxy[i] == 1:\n",
    "        pos_total += 1\n",
    "        if y_pred_proxy_sampled[i] == y_test_proxy[i]:\n",
    "            pos_correct += 1\n",
    "    else:\n",
    "        neg_total += 1\n",
    "        if y_pred_proxy_sampled[i] == y_test_proxy[i]:\n",
    "            neg_correct += 1\n",
    "\n",
    "print(\"Positive accuracy: \", pos_correct / pos_total)\n",
    "print(\"Negative accuracy: \", neg_correct / neg_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "# put more negative samples into training data\n",
    "new_negative_samples = []\n",
    "new_positive_samples = []\n",
    "for x, y in zip(X_train_proxy[:][min(num_samples_idx*10,X_len):], y_train_proxy[:][min(num_samples_idx*10,y_len):]):\n",
    "    if y == 0:\n",
    "        new_negative_samples = scipy.sparse.vstack([new_negative_samples, x]).tocsr()\n",
    "    else:\n",
    "        new_positive_samples = scipy.sparse.vstack([new_positive_samples, x]).tocsr()\n",
    "print(new_negative_samples.shape, new_positive_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_accuracy, new_pos_accuracy, new_neg_accuracy = [accuracy], [pos_correct / pos_total], [neg_correct / neg_total]\n",
    "num_new_samples = 10\n",
    "for k in range(num_new_samples):\n",
    "    X_train_proxy_sampled_plus = scipy.sparse.vstack([X_train_proxy[:min(num_samples_idx*10,X_len)], new_negative_samples[:k]])\n",
    "    y_train_proxy_sampled_plus = y_train_proxy[:min(num_samples_idx*10,y_len)] + [0] * k\n",
    "\n",
    "    # Train a logistic regression classifier\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train_proxy_sampled_plus, y_train_proxy_sampled_plus)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_proxy_sampled_plus = classifier.predict(X_test_proxy)\n",
    "\n",
    "    # Evaluate the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test_proxy, y_pred_proxy_sampled_plus)\n",
    "    new_accuracy.append(accuracy)\n",
    "\n",
    "    # calculate accuracy for each class\n",
    "    pos_correct = 0\n",
    "    pos_total = 0\n",
    "    neg_correct = 0\n",
    "    neg_total = 0\n",
    "    for i in range(len(y_pred_proxy_sampled_plus)):\n",
    "        if y_test_proxy[i] == 1:\n",
    "            pos_total += 1\n",
    "            if y_pred_proxy_sampled_plus[i] == y_test_proxy[i]:\n",
    "                pos_correct += 1\n",
    "        else:\n",
    "            neg_total += 1\n",
    "            if y_pred_proxy_sampled_plus[i] == y_test_proxy[i]:\n",
    "                neg_correct += 1\n",
    "                \n",
    "    new_pos_accuracy.append(pos_correct / pos_total)\n",
    "    new_neg_accuracy.append(neg_correct / neg_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the accuracy changes as we adjust the number of samples\n",
    "plt.plot(np.arange(num_samples_idx*10, num_samples_idx*10+num_new_samples+1), new_accuracy)\n",
    "plt.plot(np.arange(num_samples_idx*10, num_samples_idx*10+num_new_samples+1), new_pos_accuracy)\n",
    "plt.plot(np.arange(num_samples_idx*10, num_samples_idx*10+num_new_samples+1), new_neg_accuracy)\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# set x-axis split\n",
    "plt.xticks(np.arange(num_samples_idx*10, num_samples_idx*10+num_new_samples+1, 1))\n",
    "# set size\n",
    "plt.gcf().set_size_inches(10, 4)\n",
    "plt.savefig('accuracy_change.png', dpi=300)\n",
    "plt.legend([\"Overall\", \"Positive\", \"Negative\"])\n",
    "print(new_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8849104859335039\n",
      "class 0, Accuracy: 85.26%\n",
      "class 1, Accuracy: 96.12%\n",
      "class 2, Accuracy: 82.95%\n",
      "class 3, Accuracy: 88.57%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_gold, X_test_gold, y_train_gold, y_test_gold = train_test_split(filtered_texts, filtered_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_gold = vectorizer.fit_transform(X_train_gold)\n",
    "X_test_gold = vectorizer.transform(X_test_gold)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_gold, y_train_gold)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gold = classifier.predict(X_test_gold)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test_gold, y_pred_gold)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# calculate accuracy for each class\n",
    "for i in range(4):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j in range(len(y_pred_gold)):\n",
    "        if y_test_gold[j] == i:\n",
    "            total += 1\n",
    "            if y_pred_gold[j] == y_test_gold[j]:\n",
    "                correct += 1\n",
    "    print(\"class {}, Accuracy: {:.2f}%\".format(i, correct / total*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
