{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lynie/miniconda3/envs/eff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_config = ModelArguments()\n",
    "lr_config.num_labels = 4\n",
    "lr_config.cache_size = 8\n",
    "lr_config.cost = 1 #110M for bert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA Accuracy: 0.6866\n",
      "LLAMA Accuracy: 1.0\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.1, Accuracy: 0.6252\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.15000000000000002, Accuracy: 0.626\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.20000000000000004, Accuracy: 0.6614\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.25000000000000006, Accuracy: 0.6366\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.30000000000000004, Accuracy: 0.6922\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.3500000000000001, Accuracy: 0.675\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.40000000000000013, Accuracy: 0.6512\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.45000000000000007, Accuracy: 0.6622\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.5000000000000001, Accuracy: 0.6678\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.5500000000000002, Accuracy: 0.6536\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.6000000000000002, Accuracy: 0.674\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.6500000000000001, Accuracy: 0.675\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.7000000000000002, Accuracy: 0.6788\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.7500000000000002, Accuracy: 0.6434\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.8000000000000002, Accuracy: 0.6758\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.8500000000000002, Accuracy: 0.6754\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.9000000000000002, Accuracy: 0.6596\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.9500000000000003, Accuracy: 0.6512\n"
     ]
    }
   ],
   "source": [
    "# split into train, val, test\n",
    "set_seed(42)\n",
    "data = datasets.load_from_disk(\"./data/10000_sampled_agnews\")\n",
    "\n",
    "llama_labels = open(\"./llama_results/10000_sampled_agnews_llama.txt\", \"r\").readlines()\n",
    "llama_labels = [int(l.strip()) for l in llama_labels]\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['label'] == llama_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"LLAMA Accuracy: {correct/total}\")\n",
    "\n",
    "def update_labels(example, idx):\n",
    "    example['llm_label'] = llama_labels[idx]\n",
    "    return example\n",
    "\n",
    "data = data.map(update_labels, with_indices=True)\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['llm_label'] == llama_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"LLAMA Accuracy: {correct/total}\")\n",
    "\n",
    "# split data into train and test\n",
    "original_data = data.train_test_split(test_size=0.5)\n",
    "\n",
    "original_train_data = original_data['train'].shuffle()\n",
    "\n",
    "results = []\n",
    "for i in np.arange(0.1, 1, 0.05):\n",
    "    train_data = original_train_data.train_test_split(train_size=i, shuffle=False)\n",
    "    lr_model = lr.LogisticRegressionModelSkLearn(lr_config, data=train_data['train']['text'])\n",
    "\n",
    "    # train model\n",
    "    lr_model.train(train_data['train'])\n",
    "    acc = lr_model.evaluate(original_data['test'])\n",
    "    print(f\"Train Size: {i}, Accuracy: {acc}\")\n",
    "    results.append((i, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1, 0.6252),\n",
       " (0.15000000000000002, 0.626),\n",
       " (0.20000000000000004, 0.6614),\n",
       " (0.25000000000000006, 0.6366),\n",
       " (0.30000000000000004, 0.6922),\n",
       " (0.3500000000000001, 0.675),\n",
       " (0.40000000000000013, 0.6512),\n",
       " (0.45000000000000007, 0.6622),\n",
       " (0.5000000000000001, 0.6678),\n",
       " (0.5500000000000002, 0.6536),\n",
       " (0.6000000000000002, 0.674),\n",
       " (0.6500000000000001, 0.675),\n",
       " (0.7000000000000002, 0.6788),\n",
       " (0.7500000000000002, 0.6434),\n",
       " (0.8000000000000002, 0.6758),\n",
       " (0.8500000000000002, 0.6754),\n",
       " (0.9000000000000002, 0.6596),\n",
       " (0.9500000000000003, 0.6512)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA Accuracy: 0.9458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 47295.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA Accuracy: 1.0\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.1, Accuracy: 0.7254\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.15000000000000002, Accuracy: 0.7978\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.20000000000000004, Accuracy: 0.8186\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.25000000000000006, Accuracy: 0.8234\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.30000000000000004, Accuracy: 0.8282\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.3500000000000001, Accuracy: 0.832\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.40000000000000013, Accuracy: 0.8356\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.45000000000000007, Accuracy: 0.8242\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.5000000000000001, Accuracy: 0.836\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.5500000000000002, Accuracy: 0.8362\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.6000000000000002, Accuracy: 0.8434\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.6500000000000001, Accuracy: 0.8294\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.7000000000000002, Accuracy: 0.8396\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.7500000000000002, Accuracy: 0.8446\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.8000000000000002, Accuracy: 0.8406\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.8500000000000002, Accuracy: 0.834\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.9000000000000002, Accuracy: 0.854\n",
      "Logistic Regression Model initialized\n",
      "Train Size: 0.9500000000000003, Accuracy: 0.8592\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "train_size=1.0000000000000004 should be either positive and smaller than the number of samples 5000 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/lynie/projects/eff_neurosymbolic/distill_lr.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/lynie/projects/eff_neurosymbolic/distill_lr.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/lynie/projects/eff_neurosymbolic/distill_lr.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m0.1\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m0.05\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/lynie/projects/eff_neurosymbolic/distill_lr.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     train_data \u001b[39m=\u001b[39m original_train_data\u001b[39m.\u001b[39;49mtrain_test_split(train_size\u001b[39m=\u001b[39;49mi, shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/lynie/projects/eff_neurosymbolic/distill_lr.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     lr_model \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39mLogisticRegressionModelSkLearn(lr_config, data\u001b[39m=\u001b[39mtrain_data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bposeidon/home/lynie/projects/eff_neurosymbolic/distill_lr.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# train model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eff/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eff/lib/python3.10/site-packages/datasets/fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/eff/lib/python3.10/site-packages/datasets/arrow_dataset.py:4403\u001b[0m, in \u001b[0;36mDataset.train_test_split\u001b[0;34m(self, test_size, train_size, shuffle, stratify_by_column, seed, generator, keep_in_memory, load_from_cache_file, train_indices_cache_file_name, test_indices_cache_file_name, writer_batch_size, train_new_fingerprint, test_new_fingerprint)\u001b[0m\n\u001b[1;32m   4392\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4393\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_size=\u001b[39m\u001b[39m{\u001b[39;00mtest_size\u001b[39m}\u001b[39;00m\u001b[39m should be either positive and smaller \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4394\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthan the number of samples \u001b[39m\u001b[39m{\u001b[39;00mn_samples\u001b[39m}\u001b[39;00m\u001b[39m or a float in the (0, 1) range\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4395\u001b[0m     )\n\u001b[1;32m   4397\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4398\u001b[0m     \u001b[39misinstance\u001b[39m(train_size, \u001b[39mint\u001b[39m)\n\u001b[1;32m   4399\u001b[0m     \u001b[39mand\u001b[39;00m (train_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n_samples \u001b[39mor\u001b[39;00m train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   4400\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(train_size, \u001b[39mfloat\u001b[39m)\n\u001b[1;32m   4401\u001b[0m     \u001b[39mand\u001b[39;00m (train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m train_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m   4402\u001b[0m ):\n\u001b[0;32m-> 4403\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4404\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_size=\u001b[39m\u001b[39m{\u001b[39;00mtrain_size\u001b[39m}\u001b[39;00m\u001b[39m should be either positive and smaller \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4405\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthan the number of samples \u001b[39m\u001b[39m{\u001b[39;00mn_samples\u001b[39m}\u001b[39;00m\u001b[39m or a float in the (0, 1) range\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4406\u001b[0m     )\n\u001b[1;32m   4408\u001b[0m \u001b[39mif\u001b[39;00m train_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(train_size, (\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m)):\n\u001b[1;32m   4409\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid value for train_size: \u001b[39m\u001b[39m{\u001b[39;00mtrain_size\u001b[39m}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(train_size)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: train_size=1.0000000000000004 should be either positive and smaller than the number of samples 5000 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "# split into train, val, test\n",
    "set_seed(42)\n",
    "train_data = datasets.load_from_disk(\"./data/5000_sampled_imdb\")\n",
    "test_data = datasets.load_from_disk(\"./data/5000_sampled_imdb_test\")\n",
    "\n",
    "llama_labels = open(\"./llama_results/10000_sampled_imdb_llama.txt\", \"r\").readlines()\n",
    "llama_labels = [int(l.strip()) for l in llama_labels]\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(train_data):\n",
    "    if train_data[i]['label'] == llama_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"LLAMA Accuracy: {correct/total}\")\n",
    "\n",
    "def update_labels(example, idx):\n",
    "    example['llm_label'] = llama_labels[idx]\n",
    "    return example\n",
    "\n",
    "train_data = train_data.map(update_labels, with_indices=True)\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(train_data):\n",
    "    if train_data[i]['llm_label'] == llama_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"LLAMA Accuracy: {correct/total}\")\n",
    "\n",
    "original_train_data = train_data.shuffle()\n",
    "\n",
    "results = []\n",
    "for i in np.arange(0.1, 8, 0.05):\n",
    "    train_data = original_train_data.train_test_split(train_size=i, shuffle=False)\n",
    "    lr_model = lr.LogisticRegressionModelSkLearn(lr_config, data=train_data['train']['text'])\n",
    "\n",
    "    # train model\n",
    "    lr_model.train(train_data['train'])\n",
    "    acc = lr_model.evaluate(test_data)\n",
    "    print(f\"Train Size: {i}, Accuracy: {acc}\")\n",
    "    results.append((i, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1, 0.7254),\n",
       " (0.15000000000000002, 0.7978),\n",
       " (0.20000000000000004, 0.8186),\n",
       " (0.25000000000000006, 0.8234),\n",
       " (0.30000000000000004, 0.8282),\n",
       " (0.3500000000000001, 0.832),\n",
       " (0.40000000000000013, 0.8356),\n",
       " (0.45000000000000007, 0.8242),\n",
       " (0.5000000000000001, 0.836),\n",
       " (0.5500000000000002, 0.8362),\n",
       " (0.6000000000000002, 0.8434),\n",
       " (0.6500000000000001, 0.8294),\n",
       " (0.7000000000000002, 0.8396),\n",
       " (0.7500000000000002, 0.8446),\n",
       " (0.8000000000000002, 0.8406),\n",
       " (0.8500000000000002, 0.834),\n",
       " (0.9000000000000002, 0.854),\n",
       " (0.9500000000000003, 0.8592)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hatespeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA Accuracy: 0.828833037466131\n",
      "LLAMA Accuracy: 1.0\n",
      "Logistic Regression Model initialized\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "data = datasets.load_from_disk(\"./data/hatespeech\")\n",
    "llama_labels = open(\"./llama_results/10000_sampled_hatespeech18_llama.txt\", \"r\").readlines()\n",
    "llama_labels = [int(l.strip()) for l in llama_labels]\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['label'] == llama_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"LLAMA Accuracy: {correct/total}\")\n",
    "\n",
    "def update_labels(example, idx):\n",
    "    example['llm_label'] = llama_labels[idx]\n",
    "    return example\n",
    "\n",
    "data = data.map(update_labels, with_indices=True)\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['llm_label'] == llama_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"LLAMA Accuracy: {correct/total}\")\n",
    "\n",
    "# split data into train and test\n",
    "data = data.train_test_split(test_size=0.5)\n",
    "\n",
    "data['train'].shuffle()\n",
    "\n",
    "results = []\n",
    "for i in np.arange(0.1, 1, 0.05):\n",
    "    train_data = data['train'].train_test_split(train_size=i, shuffle=False)\n",
    "    lr_model = lr.LogisticRegressionModelSkLearn(lr_config, data=train_data['train']['text'])\n",
    "\n",
    "    # train model\n",
    "    lr_model.train(train_data['train'])\n",
    "    acc = lr_model.evaluate(data['test'])\n",
    "    print(f\"Train Size: {i}, Accuracy: {acc}\")\n",
    "    results.append((i, acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
