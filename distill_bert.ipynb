{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lynie/miniconda3/envs/eff/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import models.lr as lr\n",
    "import models.bert as bert\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = ModelArguments()\n",
    "bert_config.num_labels = 4\n",
    "bert_config.model = \"bert-base-uncased\"\n",
    "bert_config.cache_size = 32\n",
    "bert_config.batch_size = 16\n",
    "bert_config.num_epochs = 4\n",
    "bert_config.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HATESPEECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Accuracy: 0.8334111931234234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10703/10703 [00:00<00:00, 47656.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 600 samples\n",
      "Train Size: 600, Test Size: 5352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.4222: 100%|██████████| 75/75 [00:19<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.3117: 100%|██████████| 75/75 [00:20<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.2261: 100%|██████████| 75/75 [00:20<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.0316: 100%|██████████| 75/75 [00:20<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0301: 100%|██████████| 75/75 [00:20<00:00,  3.68it/s]\n",
      "100%|██████████| 669/669 [01:07<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6439267886855241\n",
      "Validation Accuracy:  0.804745889387145\n",
      "Training with 2700 samples\n",
      "Train Size: 2700, Test Size: 5352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.1103: 100%|██████████| 338/338 [01:31<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.0468: 100%|██████████| 338/338 [01:31<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.3543: 100%|██████████| 338/338 [01:31<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.0045: 100%|██████████| 338/338 [01:30<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0024: 100%|██████████| 338/338 [01:31<00:00,  3.71it/s]\n",
      "100%|██████████| 669/669 [01:06<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7387687188019967\n",
      "Validation Accuracy:  0.8071748878923767\n",
      "Training with 4900 samples\n",
      "Train Size: 4900, Test Size: 5352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.3712: 100%|██████████| 613/613 [02:45<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.1251: 100%|██████████| 613/613 [02:45<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.0030: 100%|██████████| 613/613 [02:45<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.1090: 100%|██████████| 613/613 [02:45<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0015: 100%|██████████| 613/613 [02:45<00:00,  3.71it/s]\n",
      "100%|██████████| 669/669 [01:06<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7737104825291181\n",
      "Validation Accuracy:  0.7935351270553064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# change dataset here\n",
    "import importlib\n",
    "set_seed(42)\n",
    "data_env = 'data.hatespeech'\n",
    "data_module = importlib.import_module(data_env)\n",
    "data = datasets.Dataset.from_pandas(pd.read_csv(\"./data/hatespeech_preprocessed.csv\"))\n",
    "\n",
    "llm_labels = open(\"./gpt_results/gpt3.5/hatespeech_gpt3.5_turbo_1106.txt\", \"r\").readlines()\n",
    "llm_labels = [int(data_module.postprocess(l.strip())) for l in llm_labels]\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['label'] == llm_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"LLM Accuracy: {correct/total}\")\n",
    "\n",
    "def update_labels(example, idx):\n",
    "    example['llm_label'] = llm_labels[idx]\n",
    "    return example\n",
    "\n",
    "data = data.map(update_labels, with_indices=True)\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['llm_label'] == llm_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "assert correct/total == 1.0 # should be 1.0\n",
    "\n",
    "# split data into train and test\n",
    "data = data.shuffle()\n",
    "data = data.train_test_split(test_size=0.5)\n",
    "\n",
    "num_train = len(data['train'])\n",
    "N_range = [600, 2700, 4900]\n",
    "\n",
    "results = []\n",
    "# for i in np.arange(0.1, 0.9, 0.05):\n",
    "#     bert_model = bert.BertModel(bert_config)\n",
    "\n",
    "#     train_data = data['train'].train_test_split(train_size=i, shuffle=False)\n",
    "#     dataset = GenericDataset(train_data['train'])\n",
    "#     train_dataloader = DataLoader(dataset, batch_size=bert_config.batch_size, shuffle=True)\n",
    "\n",
    "#     val_dataset = GenericDataset(data['test'])\n",
    "#     val_dataloader = DataLoader(val_dataset, batch_size=bert_config.batch_size, shuffle=False)\n",
    "\n",
    "#     # train model\n",
    "#     acc = bert_model.train(train_dataloader, val_dataloader)\n",
    "#     results.append((i, acc))\n",
    "\n",
    "for N in N_range:\n",
    "    print(f\"Training with {N} samples\")\n",
    "    train_data = data['train'].train_test_split(train_size=N, shuffle=False)\n",
    "    print(f\"Train Size: {len(train_data['train'])}, Test Size: {len(data['test'])}\")\n",
    "    dataset = GenericDataset(train_data['train'])\n",
    "    train_dataloader = DataLoader(dataset, batch_size=bert_config.batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = GenericDataset(data['test'])\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=bert_config.batch_size, shuffle=False)\n",
    "\n",
    "    # train model\n",
    "    bert_model = bert.BertModel(bert_config)\n",
    "    acc = bert_model.train(train_dataloader, val_dataloader)\n",
    "    results.append((N, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Accuracy: 0.94152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:00<00:00, 49886.46 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1300 samples\n",
      "Train Size: 1300, Test Size: 12500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.5909: 100%|██████████| 82/82 [00:41<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.6401: 100%|██████████| 82/82 [00:42<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.0320: 100%|██████████| 82/82 [00:42<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.8381: 100%|██████████| 82/82 [00:42<00:00,  1.93it/s]\n",
      "100%|██████████| 782/782 [02:36<00:00,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7391997449386258\n",
      "Validation Accuracy:  0.8528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# split into train, val, test\n",
    "import importlib\n",
    "set_seed(42)\n",
    "data_env = 'data.imdb'\n",
    "data_module = importlib.import_module(data_env)\n",
    "data = datasets.Dataset.from_pandas(pd.read_csv(\"./data/imdb_preprocessed.csv\"))\n",
    "\n",
    "llm_labels = open(\"./gpt_results/gpt3.5/imdb_gpt3.5_turbo_1106.txt\", \"r\").readlines()\n",
    "llm_labels = [int(data_module.postprocess(l.strip())) for l in llm_labels]\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['label'] == llm_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"LLM Accuracy: {correct/total}\")\n",
    "\n",
    "def update_labels(example, idx):\n",
    "    example['llm_label'] = llm_labels[idx]\n",
    "    return example\n",
    "\n",
    "data = data.map(update_labels, with_indices=True)\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['llm_label'] == llm_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "assert correct/total == 1.0 # should be 1.0\n",
    "\n",
    "# split data into train and test\n",
    "data = data.shuffle()\n",
    "data = data.train_test_split(test_size=0.5)\n",
    "\n",
    "num_train = len(data['train'])\n",
    "# N_range = [600, 2700, 4900]\n",
    "\n",
    "results = []\n",
    "\n",
    "N_range = [1300]\n",
    "for N in N_range:\n",
    "    print(f\"Training with {N} samples\")\n",
    "    train_data = data['train'].train_test_split(train_size=N, shuffle=False)\n",
    "    print(f\"Train Size: {len(train_data['train'])}, Test Size: {len(data['test'])}\")\n",
    "    dataset = GenericDataset(train_data['train'])\n",
    "    train_dataloader = DataLoader(dataset, batch_size=bert_config.batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = GenericDataset(data['test'])\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=bert_config.batch_size, shuffle=False)\n",
    "\n",
    "    # train model\n",
    "    bert_model = bert.BertModel(bert_config)\n",
    "    acc = bert_model.train(train_dataloader, val_dataloader)\n",
    "    results.append((N, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/6512 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6512/6512 [00:00<00:00, 19961.52 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Accuracy: 0.7714987714987716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6512/6512 [00:00<00:00, 49747.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 700 samples\n",
      "Train Size: 700, Test Size: 3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.8973: 100%|██████████| 88/88 [00:23<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.5371: 100%|██████████| 88/88 [00:23<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.5423: 100%|██████████| 88/88 [00:23<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.5342: 100%|██████████| 88/88 [00:23<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0764: 100%|██████████| 88/88 [00:23<00:00,  3.73it/s]\n",
      "100%|██████████| 407/407 [00:40<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5141451414514145\n",
      "Validation Accuracy:  0.6587837837837838\n",
      "Training with 2000 samples\n",
      "Train Size: 2000, Test Size: 3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.5282: 100%|██████████| 250/250 [01:07<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.5029: 100%|██████████| 250/250 [01:07<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.4255: 100%|██████████| 250/250 [01:07<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.5288: 100%|██████████| 250/250 [01:07<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0146: 100%|██████████| 250/250 [01:07<00:00,  3.71it/s]\n",
      "100%|██████████| 407/407 [00:40<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.43726937269372695\n",
      "Validation Accuracy:  0.6566339066339066\n",
      "Training with 2800 samples\n",
      "Train Size: 2800, Test Size: 3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.3177: 100%|██████████| 350/350 [01:34<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.2785: 100%|██████████| 350/350 [01:34<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.1072: 100%|██████████| 350/350 [01:34<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.3345: 100%|██████████| 350/350 [01:34<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0147: 100%|██████████| 350/350 [01:34<00:00,  3.71it/s]\n",
      "100%|██████████| 407/407 [00:40<00:00, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.47232472324723246\n",
      "Validation Accuracy:  0.6753685503685504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "set_seed(42)\n",
    "data_env = 'data.fever'\n",
    "data_module = importlib.import_module(data_env)\n",
    "data = datasets.Dataset.from_pandas(pd.read_csv(\"./data/fever_preprocessed.csv\"))\n",
    "\n",
    "data = data.map(lambda example: {'label': 0 if example['label'] == 'REFUTES' else 1, 'text': example['text']})\n",
    "\n",
    "llm_labels = open(\"./llama_results/fever_llama2_70b_chat.txt\", \"r\").readlines()\n",
    "llm_labels = [int(l.strip()) for l in llm_labels]\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['label'] == llm_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"LLM Accuracy: {correct/total}\")\n",
    "\n",
    "def update_labels(example, idx):\n",
    "    example['llm_label'] = llm_labels[idx]\n",
    "    return example\n",
    "\n",
    "data = data.map(update_labels, with_indices=True)\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['llm_label'] == llm_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "assert correct/total == 1.0 # should be 1.0\n",
    "\n",
    "# split data into train and test\n",
    "data = data.shuffle()\n",
    "data = data.train_test_split(test_size=0.5)\n",
    "\n",
    "num_train = len(data['train'])\n",
    "# N_range = [600, 3300, 4900]\n",
    "N_range = [700, 2000, 2800]\n",
    "\n",
    "results = []\n",
    "\n",
    "for N in N_range:\n",
    "    print(f\"Training with {N} samples\")\n",
    "    train_data = data['train'].train_test_split(train_size=N, shuffle=False)\n",
    "    print(f\"Train Size: {len(train_data['train'])}, Test Size: {len(data['test'])}\")\n",
    "    # print(len(train_data['test']))\n",
    "    dataset = GenericDataset(train_data['train'])\n",
    "    train_dataloader = DataLoader(dataset, batch_size=bert_config.batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = GenericDataset(data['test'])\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=bert_config.batch_size, shuffle=False)\n",
    "\n",
    "    # train model\n",
    "    bert_model = bert.BertModel(bert_config)\n",
    "    acc = bert_model.train(train_dataloader, val_dataloader)\n",
    "    results.append((N, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  31%|███       | 2379/7666 [00:00<00:00, 23609.33 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7666/7666 [00:00<00:00, 22211.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Accuracy: 0.7034959561701017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7666/7666 [00:00<00:00, 29520.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Size:  7666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 7666/7666 [00:00<00:00, 389267.97 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Size:  7666\n",
      "Training with 1200 samples\n",
      "Train Size: 1200, Test Size: 3833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.8327: 100%|██████████| 150/150 [00:39<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.1620: 100%|██████████| 150/150 [00:40<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.5300: 100%|██████████| 150/150 [00:40<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.3735: 100%|██████████| 150/150 [00:40<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.4160: 100%|██████████| 150/150 [00:40<00:00,  3.71it/s]\n",
      "100%|██████████| 480/480 [00:47<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7166064981949458\n",
      "Validation Accuracy:  0.6196190973128098\n",
      "Training with 1500 samples\n",
      "Train Size: 1500, Test Size: 3833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.8232: 100%|██████████| 188/188 [00:50<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.5675: 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.4955: 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.4284: 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.1053: 100%|██████████| 188/188 [00:50<00:00,  3.71it/s]\n",
      "100%|██████████| 480/480 [00:47<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.8068592057761733\n",
      "Validation Accuracy:  0.6282285416123141\n",
      "Training with 2700 samples\n",
      "Train Size: 2700, Test Size: 3833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model loaded\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.8897:  24%|██▍       | 81/338 [00:22<01:10,  3.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m     71\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m bert\u001b[38;5;241m.\u001b[39mBertModel(bert_config)\n\u001b[0;32m---> 72\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m results\u001b[38;5;241m.\u001b[39mappend((N, acc))\n",
      "File \u001b[0;32m~/projects/eff_neurosymbolic/models/bert.py:70\u001b[0m, in \u001b[0;36mBertModel.train\u001b[0;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     69\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/eff/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eff/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_config = ModelArguments()\n",
    "bert_config.num_labels = 7\n",
    "bert_config.model = \"bert-base-uncased\"\n",
    "bert_config.cache_size = 16\n",
    "bert_config.batch_size = 8\n",
    "bert_config.num_epochs = 5\n",
    "bert_config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import importlib\n",
    "data_env = 'data.isear'\n",
    "data_module = importlib.import_module(data_env)\n",
    "\n",
    "set_seed(42)\n",
    "data = datasets.Dataset.from_pandas(pd.read_csv(\"./data/isear_preprocessed.csv\"))\n",
    "\n",
    "isear_to_id = data_module.isear_to_id\n",
    "\n",
    "# Change labels to id\n",
    "data = data.map(lambda e: {'label': isear_to_id[e['label']]})\n",
    "\n",
    "# llm_labels = open(\"./llama_results/isear_llama2_70b_chat.txt\", \"r\").readlines()\n",
    "# llm_labels = [int(l.strip()) for l in llm_labels]\n",
    "llm_labels = open(\"./gpt_results/gpt3.5/isear_gpt3.5_turbo_1106.txt\", \"r\").readlines()\n",
    "llm_labels = [int(data_module.postprocess(l.strip())) for l in llm_labels]\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['label'] == llm_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(f\"LLM Accuracy: {correct/total}\")\n",
    "\n",
    "def update_labels(example, idx):\n",
    "    example['llm_label'] = llm_labels[idx]\n",
    "    return example\n",
    "\n",
    "data = data.map(update_labels, with_indices=True)\n",
    "total, correct = 0, 0\n",
    "for i, d in enumerate(data):\n",
    "    if data[i]['llm_label'] == llm_labels[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "assert correct/total == 1.0 # should be 1.0\n",
    "\n",
    "# filter out labels that are -1\n",
    "print(\"Original Size: \", len(data))\n",
    "data = data.filter(lambda e: e['llm_label'] != -1)\n",
    "print(\"Filtered Size: \", len(data))\n",
    "\n",
    "# split data into train and test\n",
    "data = data.shuffle()\n",
    "data = data.train_test_split(test_size=0.5)\n",
    "\n",
    "num_train = len(data['train'])\n",
    "# N_range = [600, 3300, 4900]\n",
    "N_range = [1200, 1500, 2700]\n",
    "\n",
    "results = []\n",
    "\n",
    "for N in N_range:\n",
    "    print(f\"Training with {N} samples\")\n",
    "    train_data = data['train'].train_test_split(train_size=N, shuffle=False)\n",
    "    print(f\"Train Size: {len(train_data['train'])}, Test Size: {len(data['test'])}\")\n",
    "    # print(len(train_data['test']))\n",
    "    dataset = GenericDataset(train_data['train'])\n",
    "    train_dataloader = DataLoader(dataset, batch_size=bert_config.batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = GenericDataset(data['test'])\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=bert_config.batch_size, shuffle=False)\n",
    "\n",
    "    # train model\n",
    "    bert_model = bert.BertModel(bert_config)\n",
    "    acc = bert_model.train(train_dataloader, val_dataloader)\n",
    "    results.append((N, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1, 0.86856),\n",
       " (0.15000000000000002, 0.8992),\n",
       " (0.20000000000000004, 0.85496),\n",
       " (0.25000000000000006, 0.89752),\n",
       " (0.30000000000000004, 0.89936),\n",
       " (0.3500000000000001, 0.90336),\n",
       " (0.40000000000000013, 0.91104),\n",
       " (0.45000000000000007, 0.90792),\n",
       " (0.5000000000000001, 0.9116),\n",
       " (0.5500000000000002, 0.91672),\n",
       " (0.6000000000000002, 0.89664),\n",
       " (0.6500000000000001, 0.89808),\n",
       " (0.7000000000000002, 0.8888),\n",
       " (0.7500000000000002, 0.90968),\n",
       " (0.8000000000000002, 0.89928),\n",
       " (0.8500000000000002, 0.91336)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
